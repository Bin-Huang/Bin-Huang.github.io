[{"categories":null,"contents":"","date":"Feb 15","permalink":"/projects/wml/","tags":null,"title":"WhereMyLife"},{"categories":null,"contents":"","date":"Feb 15","permalink":"/projects/wml-blog/","tags":null,"title":"WhereMyLife Blog"},{"categories":null,"contents":"","date":"Nov 03","permalink":"/projects/jk-broadcast/","tags":null,"title":"即刻镇广播"},{"categories":null,"contents":"","date":"Nov 03","permalink":"/projects/prray/","tags":null,"title":"Prray"},{"categories":null,"contents":"我追逐一个线上问题的幽灵，直到淹没在迷惑的日志森林里。我在脑海中苦苦思索，身后一声惊奇的鸦叫，我猛然发现当作指南针的 Sentry 报告，才是真正的迷惑来源……\n   发现问题  我们项目中使用 Sentry 来捕获错误，顺便会附带一些上下文，比如用户 ID、请求链路、版本环境……这些上下文在排查时非常有用，然而最近却误导了我，让我在错误的方向上花了不少时间，回过头来才发现上下文是有误的。\n具体来说，原本是用户 A 触发了某个错误，Sentry 上报错误的上下文里却显示是用户 B 触发的。我进一步发现，这种 Sentry 误报并不是稳定出现，而是随机出现在项目几乎所有种类的错误报告里。出于直觉，我认为这很可能是一个并发冲突问题：比如 Sentry 在报告上下文时出现了并发冲突。\n   实验模拟  下面是一段最简单的实验代码：我们启动了两个并发的协程、分别不断地上报自己的错误，然后观察两个协程的错误上下文会不会互相干扰。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  // 协程一：不断上报错误 test-sentry-1，并附带上下文 {Username: \u0026#34;1\u0026#34;} go func() { for { sentry.WithScope(func(scope *sentry.Scope) { scope.SetUser(sentry.User{Username: \u0026#34;1\u0026#34;}) sentry.CaptureException(errors.New(\u0026#34;test-sentry-1\u0026#34;)) }) } }() // 协程二：不断上报错误 test-sentry-2，并附带上下文 {Username: \u0026#34;2\u0026#34;} go func() { for { sentry.WithScope(func(scope *sentry.Scope) { scope.SetUser(sentry.User{Username: \u0026#34;2\u0026#34;}) sentry.CaptureException(errors.New(\u0026#34;test-sentry-2\u0026#34;)) }) } }()   很快我们就会发现，在 Sentry 上报的错误中，已经随机出现了错误上下文混乱的情况。比如在下图，在代码中上报错误 test-sentry-2 附带了 {Username: \u0026quot;2\u0026quot;} 的上下文信息，但 Sentry 实际却上报了 {Username: \u0026quot;1\u0026quot;} 的上下文。很明显这个错误的上下文被其他并发协程影响了。\n抓个正着！看来 Sentry 报告上下文时确实出现了并发冲突（至少目前看来如此……）。\n   理论原因  查看 sentry-go 的源码，发现实现代码里常常出现这两个结构体 Scope 和 Hub。\n Scope：即上下文，存放为错误追加的额外信息，比如用户 ID、请求 ID、请求路径、附加标签…… Hub：也许可以叫“捕获器”，内部维护了一个上下文堆栈。  Hub.stack：一个堆栈，由多层 layer 组成 Hub.stack[0].client：每一层包含服务连接实例 Hub.stack[0].scope：每一层也可能包含当前上下文    当我们上报错误时，每次执行 sentry.CaptureException(err)，实际上是从当前 hub 中获取最新的上下文（一般位于 stack 顶层，若当前层没有则查找下一层），然后与错误一起通过服务连接上报。\n而添加/删除上下文时，可以用 hub.PushScope() 或 hub.PopScope() 方法，即为 hub 的内部堆栈压入一个包含上下文信息的新堆栈层，或者弹出不再需要的层。我们一般用 WithScope 方法来附加一些临时的上下文。这个方法的原理是执行传入函数，在传入函数执行前自动 push，在执行后自动 pop。\n1 2 3 4 5  // WithScope 方法的使用案例 sentry.WithScope(func(scope *sentry.Scope) { scope.SetUser(sentry.User{Username: \u0026#34;1\u0026#34;}) sentry.CaptureException(errors.New(\u0026#34;test-sentry-1\u0026#34;)) })   其中 WithScope 的源码实现：\n1 2 3 4 5 6  // Sentry 的 WithScope 方法源码 func (hub *Hub) WithScope(f func(scope *Scope)) { scope := hub.PushScope() defer hub.PopScope() f(scope) }   通过阅读源码，很明显 WithScope 这个方法在并发下不安全。如果一个协程用 WithScope 刚刚压入新层、正在编辑上下文，然后另一个协程也压入了新层，那么就会出现并发冲突，比如污染其他协程的上下文、或者用其他协程的上下文来上报自己的错误！\n   设计如此？  难道要怪 Sentry 客户端库存在漏洞？不，设计如此，或者更像是迫不得已。\nSentry 基于 Scope Stack 的设计可以很方便地继承和拓展上下文，尤其在程序不同层级之间很有用。因为 Sentry 不是只面向 Golang 这门语言，还需要支持 Node.js、Python、PHP、C#……所以 Sentry 必须要在所有语言中尽可能做到接口统一，像 push、pop、withScope 这类操作都是统一提供的。然而在 Golang 中，要让 WithScope 方法做到全局并发安全几乎不太现实。因为如果加锁的话，WithScope 的错误上报只能串行执行了，严重影响了上报性能。\n   规避方法  既然全局并发安全做不到，那么局部并发安全还是可以有的。只要每个协程都有一个自己专属的上下文堆栈，那么就不用担心互相污染的问题。为此 Sentry 专门提供了 hub.Clone() 方法。\n1 2 3 4 5 6 7  go func () { hub := sentry.CurrentHub().Clone() // 获得新 hub \thub.WithScope(func(scope *sentry.Scope) { // 为新 hub 添加上下文 \tscope.SetTag(\u0026#34;action\u0026#34;, \u0026#34;produce\u0026#34;) hub.CaptureException(err) // 用新 hub 上报错误 \t}) }()   Clone() 方法会引用当前 hub 的底层服务连接，并复制一份当前的上下文堆栈的拷贝，然后生成一个新的 hub 实例。因为复用了底层服务连接，很明显 hub 克隆的成本很低，可以随用随抛。因此在实践中，不管是否存在并发，上报错误时都最好克隆一下 hub 实例。\n这个方法可以完美地规避并发冲突问题，nice~\n","date":"Mar 21","permalink":"/posts/sentry-hub-scope/","tags":null,"title":"Sentry 高并发时上报异常的问题排查"},{"categories":null,"contents":"   前言  众所周知，ElasticSearch 中的 Mapping 决定了字段的索引方式。当一个字段的索引类型确定后就无法修改。然而在日常开发中，忘记更新 mapping 是偶尔会遇到的，但修改 mapping 却是一件不容易的事情。这篇文章不仅详细讨论了 reindex 这种常见解决方法，包括 reindex 的停机和数据丢失问题、以及在各种情况下规避上述问题的思路和方法，还介绍了另外两种更加低成本的解决方法。\n   一个常见的错误  “发布测试环境前我竟然忘记修改 mapping 了？”\n{\u0026quot;error\u0026quot;:{\u0026quot;root_cause\u0026quot;:[{\u0026quot;type\u0026quot;:\u0026quot;illegal_argument_exception\u0026quot;,\u0026quot;reason\u0026quot;:\u0026quot;Fielddata is disabled on text fields by default. Set fielddata=true on [player_type] in order to load fielddata in memory by uninverting the inverted index...... 所以我要如何快速解决这个问题呢？ 所以我要如何快速修改 mapping 中一个字段的索引类型呢？\n   Reindex  “即使对 reindex 烂熟于心，也需要留意停机时间和数据丢失……”\nReindex 几乎是这类问题的标准答案，其中一个常见思路是：创建一个新的 index-tmp，然后把数据 reindex 到这个新 index-tmp，再把原来的 index 删除重建、设置正确的 mapping，最后把数据重新从 index-tmp 中 reindex 回来。很明显这个思路除了操作危险外，还会有较长的停机时间。\nElasticSearch 官方博客里曾介绍过另一种方法，只要后端业务以 alias 的方式访问 index，就可以做到完全无停机时间（Zero-Downtime）。具体思路是：首先创建一个 mapping 正确的新 index-v2，把数据 reindex 到这个新 index-v2，然后直接把 alias 重命名到这个index-v2。这样可以让后端业务直接访问到新的 index-v2，达到后端业务对 reindex 操作无感知的效果。\n然而这种方法也有不足，官方博客里完全没有提及数据丢失的问题。如果线上写入操作频繁，在 reindex 和更新 alias 步骤中有一段时间窗口，新数据依然会写入到即将弃用的旧 index，造成新 index-v2 丢失该部分数据的问题。即使采用官博推荐的 bulk API，也无法在原理上根本地规避问题。\n因此需要结合实际的业务情况，做一些因地制宜的权衡。\n1. 那就丢亿点点数据\n有时候，丢失一些临时数据也是可以接受的，比如测试环境的PV上报、用来时间窗口统计的热词记录……对于这些可丢失数据，当我们发出那句灵魂拷问，“数据重要还是我重要下班重要！”，相信产品经理也是会认真考虑的。\n2. 找个夜黑风高的时候停机\n防止新数据丢失最简单的办法就是停机了。只要没有写入，哪来数据丢失。在我常常遇到的 ES 使用场景里，新数据总是通过消费者从其他数据源里同步过来，只要先让消费者暂停就可以解决问题。后端业务还是可以正常读取 ES，只是停止了消费者的写入。当一切结束后，消费者可以直接从消息队列里获取堆积的写入任务，轻松地恢复工作。\n3. 让新数据先走\n能不能在不停机的情况下，也能做到不丢失数据？完全可以。创建新 index 后，立即重新设置 alias，让新 index 先开始接受新数据，然后再把老数据 reindex 过来。这样数据要么在新 index、要么在旧 index，但最终都会写入在新 index，解决了新文档丢失的问题。\n但我也想到了这种方法的几个不足和局限：\n 中间会有很长一段时间（当新 index 开始接客、旧数据还没有全部 reindex 过来时），后端业务将无法访问到旧数据。这可能会对业务造成影响，比如用户突然无法搜索到以前的文章。 这个方法只能保证新文档插入不丢失，对旧文档的更新和删除操作依然可能丢失。当然，如果业务里 ES 的使用场景是“只写不改”，那也就不是问题了。不过说起来，把 ES 用来作为 CRUD 数据库确实是很奇怪的技术选型～  4. 两步 reindex\n我还想到了一个方法，可以在不停机的情况下，既让后端业务尽可能正常地访问老数据，又不丢失新文档的创建。这个方法要求文档创建时附带一个 created_at 字段。\n 创建一个新的 index-v2，设置好需要的 mapping。  PUT /index-v2/_doc/_mapping { \u0026quot;properties\u0026quot;: \u0026lt;all_you_need\u0026gt; } 先“悄悄”准备老数据，比如先 reindex 某个时间点之前的老数据（比如一小时前）。  POST _reindex { \u0026quot;source\u0026quot;: { \u0026quot;index\u0026quot;: \u0026quot;index-v1\u0026quot;, \u0026quot;query\u0026quot;: { \u0026quot;range\u0026quot;: { \u0026quot;created_at\u0026quot;: { \u0026quot;lt\u0026quot;: \u0026lt;一个时间点\u0026gt; } } } }, \u0026quot;dest\u0026quot;: { \u0026quot;index\u0026quot;: \u0026quot;index-v2\u0026quot; } } 当老数据准备好后，立即修改 alias，让新 index-v2 开始接受新数据。  POST /_aliases { \u0026quot;actions\u0026quot;: [ { \u0026quot;remove\u0026quot;: { \u0026quot;alias\u0026quot;: \u0026quot;index\u0026quot;, \u0026quot;index\u0026quot;: \u0026quot;index-v1\u0026quot; } }, { \u0026quot;add\u0026quot;: { \u0026quot;alias\u0026quot;: \u0026quot;index\u0026quot;, \u0026quot;index\u0026quot;: \u0026quot;index-v2\u0026quot; } } ] } 再将这个时间点之后的数据 reindex 到新 index-v2  POST _reindex { \u0026quot;source\u0026quot;: { \u0026quot;index\u0026quot;: \u0026quot;index-v1\u0026quot;, \u0026quot;query\u0026quot;: { \u0026quot;range\u0026quot;: { \u0026quot;created_at\u0026quot;: { \u0026quot;gte\u0026quot;: \u0026lt;一个时间点\u0026gt; } } } }, \u0026quot;dest\u0026quot;: { \u0026quot;index\u0026quot;: \u0026quot;index-v2\u0026quot; } } 这样在迁移过程中，既不需要停机，也不会丢失新数据，也不会出现长时间无法访问历史数据的问题。后端业务几乎可以访问到所有的数据，只有在第二次 reindex 期间会有极小部分数据在短时间内无法访问（不可见数据范围大约是 created_at: [the_timestamp, now）），但也会很快地恢复。因为数据量非常少，第二次 reindex 速度很快，等到结束后，后端业务就可以完全正常地访问到所有的数据。\n这个方法可以保证新文档插入不丢失、大部分旧文档更新删除不丢失，但在第二次 reindex 期间那些不可见文档的更新、删除操作依然有丢失的可能。这个方法在不停机的前提下大幅地缩小了负面影响的范围。\n小结\n以上只是尽可能地讨论了 reindex 在实际业务中可能遇到的问题，以及可行的方法与思路。虽然在实际应用中，大多数情况都可以用简单直接的方法解决，比如遗弃数据和停机，但认识更多情况可以规避潜在的决策风险。\n对了，这里没有讨论 reindex 优化，我感觉这是另外一个话题了。我看到有些高赞文章提到通过配置来增大 reindex 单次索引的文档数量，或者通过 scroll 并发来加快索引。我直观的感觉，这些方法在停机情况下应该是不错的做法，但在不停机时可能会给集群带来更多的压力，尤其是 index 数据量很大、读写频繁的场景，可能会影响到正常的业务。在不停机的情况，也许我们应该更多考虑的不是如何加快 reindex 过程，而是尽可能降低 reindex 对正常业务的影响。\n除了 reindex，还有其他一些方法可以“修改”已有字段的索引类型。\n   新字段替换  “既然不能直接修改原来字段的索引类型，那我重新建个字段好吧……”\n很多时候，为了一点点错误索引的数据而 reindex 整个 index，也许是一件大动干戈的事情。利用其他技巧可以更加低成本的解决问题，比如建个新字段。\n 在 mapping 添加新字段 new_field，设置好需要的索引方式  PUT /my-index/_doc/_mapping { \u0026quot;properties\u0026quot;: { \u0026quot;new_field\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot; } } } 将原来文档的旧字段的值重新赋予到新字段  POST /my-index/_update_by_query?conflicts=proceed { \u0026quot;script\u0026quot;: { \u0026quot;source\u0026quot;: \u0026quot;ctx._source.new_field = ctx._source.old_field\u0026quot;, \u0026quot;lang\u0026quot;: \u0026quot;painless\u0026quot; }, \u0026quot;query\u0026quot;: { \u0026quot;exists\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;old_field\u0026quot; } } } 在业务代码中弃用原来的字段  小结\n这个方法非常适合低成本解决少量数据被错误索引的情况，比如在发布前忘记修改 mapping。但是如果需要修改索引类型的数据很多，这个方法需要更新大量已有数据，可能会对集群带来一定压力。毕竟在 ElasticSearch 里，所谓的文档修改就是先标记删除、然后重新插入，如果要更新整个 index 的所有文档，也许还不如 reindex 来得痛快……\n   采用 multi-field  \u0026ldquo;作为一个成熟的 DB，总是可以为一个字段建立多种索引……\u0026quot;\nElasticSearch 的 multi-field 特性，让同一个字段可以有多种不同的索引类型。我们可以利用这个特性在 mapping 中为老字段追加新的索引类型。比如这个例子：\nPUT /my-index/_doc/_mapping { \u0026quot;properties\u0026quot;: { \u0026quot;company\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot;, \u0026quot;fields\u0026quot;: { \u0026quot;name\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot; } } } } } 原本 company 字段采用了 keyword 的索引类型，只能用于数值匹配。在上面的例子中，为 company 字段追加了另一种支持全文搜索的索引类型 text，然后把这个全文搜索版本取名叫 company.name。这样查询条件 { \u0026quot;company.name\u0026quot;: \u0026quot;腾讯\u0026quot; } 就可以匹配到 { \u0026quot;company\u0026quot;: \u0026quot;腾讯科技有限公司\u0026quot; } 的数据了。\n因为本质上只是对同一个字段添加不同的索引，实际文档（documents）中并不会真的多出来一个叫 company.name 的字段，插入数据时也不需要为 company.name 赋值。一切索引工作都由 ElasticSearch 自动完成。\n然而用这种方法修改 mapping 后，只有新的写入才会让该文档的新索引生效。也就是说，只有插入新的文档、或者更新旧文档时，ElasticSearch 才会给该文档重新索引、将该文档写入新索引。除非老数据有更新，用 company.name 无法搜索到老数据，因为老数据根本不在这个索引里！\n但问题总是可以解决，不就是还差一次全局更新嘛～（坏笑脸\n// 这个更新没有任何意义，纯粹是为了触发老数据的重新索引 POST /my-index/_update_by_query?conflicts=proceed { \u0026quot;script\u0026quot;: { \u0026quot;source\u0026quot;: \u0026quot;ctx\u0026quot;, \u0026quot;lang\u0026quot;: \u0026quot;painless\u0026quot; }, \u0026quot;query\u0026quot;: { \u0026quot;exists\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;company\u0026quot; } } } 小结\n这个方法不仅适合低成本解决少量数据被错误索引的情况，而且在同个字段需要多种索引方式（尤其需要是搭配不同的分析器）时这几乎是唯一的做法，因为 multi-field 就是为了这种场景准备的。但这个方法同样需要注意更新数据的规模，以及对性能的影响……\n   最后  我在中英互联网上搜索时，发现很难找到详细讨论这个话题的文章。其中我找到最好的资料是 ElasticSearch 官博的一篇文章，里面简要介绍了这三种方法，但是很少介绍各个方法的局限、已知问题和适用场景，比如几乎没有提到 reindex 的数据丢失问题。我还找到了很多只言片语，它们大多只是简单提供了一个解决方法，却很少介绍方法的用意和局限。\n所以我尝试写篇文章，想着结合自己的认识和理解，试着更加全面地讨论一下各个方法的利弊权衡，看看能不能从这个常见的小问题出发，窥探到一点点原理和设计的影子，填补一点点底层技术与实际业务的缝隙，引发一些可能的讨论和思路。\n最后不得不感叹：方法常有，银弹难寻！只有根据实际情况，在业务可接受的影响范围内，找到最简单方法，往往那就是最佳实践。\n   参考   https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-reindex.html https://www.elastic.co/guide/en/elasticsearch/reference/current/multi-fields.html https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-put-mapping.html#add-multi-fields-existing-field-ex  ","date":"Feb 14","permalink":"/posts/es-mapping/","tags":null,"title":"深入讨论几种 ES Mapping 修改方法和局限"},{"categories":null,"contents":"","date":"Jan 01","permalink":"/articles/","tags":null,"title":"文章"}]